{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToDo \n",
    "\n",
    "Most of work is done and only the following needs to be done\n",
    "\n",
    "AutoUnit-**trainging step**  https://pytorch.org/tnt/stable/framework/auto_unit.html\n",
    "- add login W&B\n",
    "- add validation step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "\n",
    "# File paths to save/load the datasets\n",
    "train_file = 'training_dataset.pt'\n",
    "valid_file = 'validation_dataset.pt'\n",
    "\n",
    "\n",
    "from torchtnt.framework.auto_unit import AutoUnit\n",
    "from torchtnt.framework.fit import fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(fit)\n",
    "wandb.init(project=\"my_transformer_project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"SouthernCrossAI/Project_Gutenberg_Australia\"\n",
    "sequence_length = 512\n",
    "batch_size = 16\n",
    "\n",
    "# File paths to save/load the datasets after first trasnsfermations\n",
    "train_file = 'training_dataset.pt'\n",
    "valid_file = 'validation_dataset.pt'\n",
    "\n",
    "# Model Configuration\n",
    "vocab_size = 50257\n",
    "n_embd = 512\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "n_layer_decoder = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ds['train'].train_test_split(test_size=0.2)\n",
    "print(f\"test = {len(dataset['test'])} and train = {len(dataset['train'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer and function to prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', clean_up_tokenization_spaces=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(dataset):\n",
    "    return tokenizer(dataset['Paragraph'], \n",
    "                    truncation=True, \n",
    "                    padding='max_length', \n",
    "                    max_length=sequence_length,\n",
    "                    return_attention_mask=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if transformed datasets already exist\n",
    "if os.path.exists(train_file) and os.path.exists(valid_file):\n",
    "    # Load the transformed datasets\n",
    "    training_dataset = torch.load(train_file)\n",
    "    validation_dataset = torch.load(valid_file)\n",
    "    print(\"Loaded existing transformed datasets.\")\n",
    "else:\n",
    "    # Create the transformed datasets\n",
    "    training_dataset = dataset['train'].map(tokenize_function, batched=True)\n",
    "    validation_dataset = dataset['test'].map(tokenize_function, batched=True)\n",
    "    \n",
    "    # Set the format for PyTorch\n",
    "    training_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "    validation_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "    \n",
    "    # Save the transformed datasets for future use\n",
    "    torch.save(training_dataset, train_file)\n",
    "    torch.save(validation_dataset, valid_file)\n",
    "    print(\"Transformed datasets created and saved.\")\n",
    "\n",
    "# Old Code# Check if transformed datasets already exist\n",
    "if os.path.exists(train_file) and os.path.exists(valid_file):\n",
    "    # Load the transformed datasets\n",
    "    training_dataset = torch.load(train_file)\n",
    "    validation_dataset = torch.load(valid_file)\n",
    "    print(\"Loaded existing transformed datasets.\")\n",
    "else:\n",
    "    # Create the transformed datasets\n",
    "    training_dataset = dataset['train'].map(tokenize_function, batched=True)\n",
    "    validation_dataset = dataset['test'].map(tokenize_function, batched=True)\n",
    "    print(\"Transformed datasets created and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 20 \n",
    "train_indices = torch.randperm(len(training_dataset))[:num_samples]\n",
    "valid_indices = torch.randperm(len(validation_dataset))[:num_samples]\n",
    "\n",
    "\n",
    "small_train_subset = Subset(training_dataset, train_indices)\n",
    "small_valid_subset = Subset(validation_dataset, valid_indices)\n",
    "training_dataloader = DataLoader(small_train_subset, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(small_valid_subset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly, for the validation DataLoader\n",
    "for batch in validation_dataloader:\n",
    "    print(\"Batch input_ids shape:\", batch['input_ids'].shape)\n",
    "    print(\"Batch attention_mask shape:\", batch['attention_mask'].shape)\n",
    "    break  # Exit after printing the size of the first batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contextual embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding = nn.Embedding(sequence_length, n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tokens = self.token_embedding(x)\n",
    "        positions = torch.arange(x.size(1), device=x.device).unsqueeze(0).expand_as(x)\n",
    "        positions = self.position_embedding(positions)\n",
    "        x = tokens + positions\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trasformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the TransformerBlock to use key_padding_mask\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attn = nn.MultiheadAttention(n_embd, n_head)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd)\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None):\n",
    "        # Transpose for MultiheadAttention (seq_len, batch_size, embed_dim)\n",
    "        x = x.transpose(0, 1)\n",
    "        attn_output, _ = self.attn(x, x, x, key_padding_mask=key_padding_mask)\n",
    "        x = x + attn_output\n",
    "        x = self.ln1(x)\n",
    "\n",
    "        # Transpose back to (batch_size, seq_len, embed_dim)\n",
    "        x = x.transpose(0, 1)\n",
    "        mlp_output = self.mlp(x)\n",
    "        x = x + mlp_output\n",
    "        x = self.ln2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BabyJoey(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BabyJoey, self).__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embeddings = Embeddings()\n",
    "        \n",
    "        # Decoder Blocks (based on n_layer_decoder)\n",
    "        self.decoder_blocks = nn.ModuleList([TransformerBlock() for _ in range(n_layer_decoder)])\n",
    "\n",
    "        # Output layer\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # Get embeddings\n",
    "        x = self.embeddings(x)\n",
    "\n",
    "        # Apply decoder blocks with attention mask\n",
    "        for block in self.decoder_blocks:\n",
    "            x = block(x, key_padding_mask=attn_mask)  # Pass attn_mask as key_padding_mask\n",
    "\n",
    "        # Layer norm and output\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BabyJoey model\n",
    "model = BabyJoey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traing Procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop Functions and Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BabyJoeyUnit(AutoUnit):\n",
    "    def __init__(self, module, device=None):\n",
    "        super().__init__(module=module, device=device)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.metrics = {}\n",
    "    \n",
    "    def compute_loss(self, state, data):\n",
    "        input_ids, attention_mask = data['input_ids'], data['attention_mask']\n",
    "        \n",
    "        # Ensure the attention mask is of type bool (for key_padding_mask)\n",
    "        key_padding_mask = (attention_mask == 0).bool()\n",
    "\n",
    "        logits = self.module(input_ids, attn_mask=key_padding_mask)\n",
    "        # Shift the input ids by one to get the target sequence\n",
    "        targets = input_ids[:, 1:].contiguous()\n",
    "        logits = logits[:, :-1, :].contiguous()\n",
    "        \n",
    "        loss = self.loss_fn(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        wandb.log({\"train_loss\": loss.item()})\n",
    "        return loss, logits\n",
    "    \n",
    "    def on_train_end(self, state):\n",
    "        torch.save(self.module.state_dict(), \"baby_joey_model.pth\")\n",
    "        print(\"模型已保存为 'baby_joey_model.pth'\")\n",
    "\n",
    "\n",
    "    def configure_optimizers_and_lr_scheduler(self, module):\n",
    "        optimizer = optim.AdamW(module.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "        return optimizer, None\n",
    "    \n",
    "    def on_eval_step_end(self, state, data, step, loss, outputs):\n",
    "        input_ids, attention_mask = data['input_ids'], data['attention_mask']\n",
    "        key_padding_mask = (attention_mask == 0).bool()\n",
    "        logits = self.module(input_ids, attn_mask=key_padding_mask)\n",
    "        targets = input_ids[:, 1:].contiguous()\n",
    "        logits = logits[:, :-1, :].contiguous()\n",
    "        loss = self.loss_fn(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        \n",
    "        self.metrics.update({'val_loss': loss.item()})\n",
    "        wandb.log({'validation_loss': loss.item()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running TNT training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtnt.framework.train import train\n",
    "\n",
    "# Correctly define the device as a torch.device object\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the BabyJoey model and move it to the device\n",
    "model = BabyJoey().to(device)\n",
    "\n",
    "# Define the custom AutoUnit with the correct device object\n",
    "baby_joey_unit = BabyJoeyUnit(module=model, device=device)\n",
    "\n",
    "# Train the model\n",
    "fit(baby_joey_unit,\n",
    "    train_dataloader=training_dataloader,\n",
    "    eval_dataloader=validation_dataloader,\n",
    "    max_epochs=2,\n",
    "    )\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
