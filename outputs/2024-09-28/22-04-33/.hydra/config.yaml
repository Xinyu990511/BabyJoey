embedding:
  vocab_size: 50257
  sequence_length: 512
  n_embd: 512
transformer:
  n_head: 8
  n_layer_decoder: 1
model:
  learning_rate: 1.0e-05
  weight_decay: 0.001
  step_size: 1
  gamma: 0.9
